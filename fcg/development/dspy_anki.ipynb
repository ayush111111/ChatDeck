{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c0d93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ConversationAnalysis(dspy.Signature):\n",
    "    \"\"\"Analyze conversation to identify learning patterns and user confusion signals\"\"\"\n",
    "    conversation = dspy.InputField()\n",
    "    user_confusion_signals = dspy.OutputField(desc=\"questions showing confusion or deep interest\")\n",
    "    concept_hierarchy = dspy.OutputField(desc=\"main concepts vs supporting details\")\n",
    "    follow_up_intensity = dspy.OutputField(desc=\"which concepts got most follow-up attention\")\n",
    "\n",
    "class ConceptPrioritization(dspy.Signature):\n",
    "    \"\"\"Rank concepts by learning importance based on conversation patterns\"\"\"\n",
    "    concepts = dspy.InputField()\n",
    "    confusion_signals = dspy.InputField()\n",
    "    follow_ups = dspy.InputField()\n",
    "    prioritized_concepts = dspy.OutputField(desc=\"concepts ranked by learning priority\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfd3819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the final signature\n",
    "class Flashcard(BaseModel):\n",
    "    \"\"\"Structure for a complete flashcard with three components\"\"\"\n",
    "    front: str\n",
    "    back: str\n",
    "    explanation: str\n",
    "\n",
    "class FlashcardFront(dspy.Signature):\n",
    "    \"\"\"Generate concise, clear flashcard fronts (questions or concepts)\"\"\"\n",
    "    concept = dspy.InputField()\n",
    "    context = dspy.InputField()\n",
    "    front = dspy.OutputField(desc=\"concise concept title or focused question\")\n",
    "\n",
    "class FlashcardBack(dspy.Signature):\n",
    "    \"\"\"Generate concise answers that directly address the flashcard front\"\"\"\n",
    "    front = dspy.InputField()\n",
    "    concept_details = dspy.InputField()\n",
    "    back = dspy.OutputField(desc=\"brief, direct answer to the front question\")\n",
    "\n",
    "class FlashcardExplanation(dspy.Signature):\n",
    "    \"\"\"Generate detailed explanations with examples to reinforce learning\"\"\"\n",
    "    front = dspy.InputField()\n",
    "    back = dspy.InputField()\n",
    "    concept_context = dspy.InputField()\n",
    "    explanation = dspy.OutputField(desc=\"detailed explanation with examples that deepens understanding\")\n",
    "\n",
    "#  Both the class docstring and the desc parameter in output fields are incorporated into the prompt that guides the LLM's generation.\n",
    "class FlashcardGeneration(dspy.Signature):\n",
    "    \"\"\"Generate flashcards following spaced repetition best practices.\n",
    "\n",
    "    Each flashcard should have:\n",
    "    1. Front: A concise concept title or focused question\n",
    "    2. Back: A brief, direct answer that addresses the front\n",
    "    3. Explanation: A detailed explanation with examples to deepen understanding. Generate detailed explanations with examples to reinforce learning. It can be long (4-5 sentences) as well.\n",
    "    \"\"\"\n",
    "    prioritized_concepts = dspy.InputField()\n",
    "    conversation_context = dspy.InputField()\n",
    "    flashcards: List[Flashcard] = dspy.OutputField(\n",
    "        desc=\"List of Flashcard models, each with front (concise concept/question), back (direct answer), and explanation (detailed with examples)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8601c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DistinctnessChecker(dspy.Signature):\n",
    "    \"\"\"Ensure flashcards cover different concepts without overlap\"\"\"\n",
    "    flashcards = dspy.InputField()\n",
    "    distinct_flashcards = dspy.OutputField(desc=\"flashcards with overlapping content merged or removed\")\n",
    "\n",
    "class RepetitionRemover(dspy.Signature):\n",
    "    \"\"\"Remove repetitive phrasing between front and back of cards\"\"\"\n",
    "    raw_flashcard = dspy.InputField()\n",
    "    clean_flashcard = dspy.OutputField(desc=\"front/back with no repeated phrasing\")\n",
    "\n",
    "class NeutralityEnforcer(dspy.Signature):\n",
    "    \"\"\"Remove references to 'user', 'AI', 'conversation' from flashcards\"\"\"\n",
    "    flashcard_with_references = dspy.InputField()\n",
    "    neutral_flashcard = dspy.OutputField(desc=\"factual flashcard without conversational references\")\n",
    "\n",
    "class CardComplexityOptimizer(dspy.Signature):\n",
    "    \"\"\"Adjust flashcard complexity based on user's demonstrated understanding level\"\"\"\n",
    "    concept = dspy.InputField()\n",
    "    user_comprehension_level = dspy.InputField()\n",
    "    optimal_flashcard = dspy.OutputField(desc=\"appropriately challenging flashcard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e3fc796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# TODO: build a timeline of how prompts evolve over time to demonstrate the value of DSPy\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "lm = dspy.LM(\n",
    "    model=\"openrouter/google/gemini-2.0-flash-001\",\n",
    "    api_base=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    ")\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Define your modules\n",
    "class ConversationToFlashcards(dspy.Module):\n",
    "    def __init__(self):\n",
    "        # Core pipeline\n",
    "        self.analyze = dspy.ChainOfThought(ConversationAnalysis)\n",
    "        self.prioritize = dspy.ChainOfThought(ConceptPrioritization)\n",
    "        self.generate = dspy.ChainOfThought(FlashcardGeneration)\n",
    "\n",
    "        # Quality control\n",
    "        # self.check_distinct = dspy.ChainOfThought(DistinctnessChecker)\n",
    "        # self.remove_repetition = dspy.ChainOfThought(RepetitionRemover)\n",
    "        # self.enforce_neutrality = dspy.ChainOfThought(NeutralityEnforcer)\n",
    "\n",
    "    def forward(self, conversation, **kwargs) -> List[Flashcard]:\n",
    "        # 1. Analyze the conversation\n",
    "        analysis = self.analyze(conversation=conversation)\n",
    "\n",
    "        # 2. Rank/prioritize concepts\n",
    "        priorities = self.prioritize(\n",
    "            concepts=analysis.concept_hierarchy,\n",
    "            confusion_signals=analysis.user_confusion_signals,\n",
    "            follow_ups=analysis.follow_up_intensity\n",
    "        )\n",
    "\n",
    "        # 3. Generate the flashcards (returns an object whose .flashcards is List[Flashcard])\n",
    "        generated = self.generate(\n",
    "            prioritized_concepts=priorities.prioritized_concepts,\n",
    "            conversation_context=conversation\n",
    "        )\n",
    "        # # Apply quality controls\n",
    "        # distinct_cards = self.check_distinct(flashcards=raw_cards.flashcards)\n",
    "\n",
    "        # # Process each card through quality pipeline\n",
    "        # final_cards = []\n",
    "        # for card in distinct_cards.distinct_flashcards:\n",
    "        #     no_repetition = self.remove_repetition(raw_flashcard=card)\n",
    "        #     neutral_card = self.enforce_neutrality(flashcard_with_references=no_repetition.clean_flashcard)\n",
    "        #     final_cards.append(neutral_card.neutral_flashcard)\n",
    "\n",
    "        return generated.flashcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d7e44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dspy\n",
    "\n",
    "# Load the conversation object from the pickle file\n",
    "file_path = r'C:\\Users\\lokar\\flashcardgen\\fcg\\development\\conversation.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    conversation = pickle.load(f)\n",
    "\n",
    "# Test individual signatures\n",
    "analyzer = dspy.ChainOfThought(ConversationAnalysis)\n",
    "result = analyzer(conversation=conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a11915a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<ChatRole.USER: 'user'>, content='GEPA algorithm,'), ChatMessage(role=<ChatRole.ASSISTANT: 'assistant'>, content='Here’s a practical breakdown of the GEPA (Genetic-Pareto) algorithm:\\n\\nWhat is GEPA?\\n\\nGEPA, short for Genetic-Pareto, is a prompt-optimization method designed to improve how large language models (LLMs) solve tasks. Unlike conventional reinforcement learning (RL) that relies on scalar rewards and requires many expensive rollouts, GEPA uses natural language reflections and evolutionary ideas to make optimization more efficient and transparent \\narXiv\\nMedium\\n.\\n\\nHow GEPA Works\\n\\nGEPA’s mechanics rely on three core ideas:\\n\\n1. Reflective Prompt Mutation\\n\\nGEPA collects execution traces—reasoning steps, tool calls, compiler error messages, evaluation logs—from the LLM system.\\n\\nAn LLM is then used to reflect on these traces, identify what failed or succeeded, and suggest improved prompt modifications in natural language \\nMedium\\narXiv\\ndspy.ai\\n.\\n\\n2. Pareto-Based Candidate Selection\\n\\nInstead of selecting only the top-performing prompt, GEPA maintains a pool of candidates that excel on at least one task instance (the Pareto frontier).\\n\\nNew candidates are stochastically sampled based on how broadly they perform well across tasks, ensuring both exploration and diversity \\narXiv\\ndspy.ai\\n.\\n\\n3. Genetic-Style Evolution\\n\\nGEPA evolves prompts by applying mutations or merging modules from different candidate lineages—akin to biological crossover and variation.\\n\\nOn each iteration:\\n\\nSelect a candidate.\\n\\nMutate its prompt using reflection.\\n\\nEvaluate on a minibatch.\\n\\nIf performance improves, add the candidate to the pool.\\n\\nContinue until the evaluation budget is exhausted \\narXiv\\ndspy.ai\\n.\\n\\nPerformance Highlights\\n\\nGEPA outperforms RL methods like GRPO by up to 20%, with up to 35× fewer rollouts \\narXiv\\nMedium\\n.\\n\\nIt also beats the previous DSPy optimizer, MIPROv2, by over 10%, while generating much shorter prompts—often up to 9× more compact \\narXiv\\nMedium\\n.\\n\\nUsing GEPA dramatically reduces the inference cost and improves sample efficiency.\\n\\nWhy It Works Better\\n\\nRich feedback: Retrieving diagnostic text instead of a single numerical score lets the algorithm understand why something failed.\\n\\nDiverse exploration: Pareto-maintained candidate pool prevents premature convergence to local optima.\\n\\nEfficiency: Fewer rollouts and shorter prompts cut down computational overhead while preserving or enhancing performance.\\n\\nTech and Implementation\\n\\nGEPA is implemented as part of the DSPy framework (a Python-based declarative system for LLM workflows) and is available on GitHub \\nGitHub\\ndspy.ai\\n.\\n\\nIt includes tools like GEPAFeedbackMetric, dspy.GEPA(...), and features like inference-time search via tracking best outputs \\ndspy.ai\\n.\\n\\nReal-World Impressions\\n\\nOn Reddit, a user shared:\\n\\n“I implemented a lightweight version of GEPA called GEPA-Lite … I got a big performance boost by giving the reflect function web tool access, and then it could actually research where it was going wrong.” \\nReddit\\n\\nAnother voice from Hacker News remarks that such algorithms are performing optimization in high noise, combinatorial spaces and highlights the need for better understanding of their scaling behavior \\nHacker News\\n.\\n\\nSummary Table\\nComponent\\tRole\\nReflective Mutation\\tUses text-based feedback to evolve prompts intelligently\\nPareto Exploration\\tKeeps a diverse set of high-performing candidate prompts\\nGenetic Evolution\\tCombines and mutates prompts using LLM-guided logic\\nEfficiency Gains\\tFewer rollouts, better performance, more compact prompts\\n\\nIn sum, GEPA is a practical, language-first optimization algorithm that delivers smarter prompt tuning—faster, cheaper, and more interpretable than traditional RL approaches.\\n\\nLet me know if you’d like a walkthrough of how to apply GEPA in code or adapt it for your workflows.'), ChatMessage(role=<ChatRole.USER: 'user'>, content='so the concepts of genetic programming applied to llm optimisation\\n\\nI know about genetic algorithms applied to hyperparameter searches \\n\\nwhat are the similiraties and differences?'), ChatMessage(role=<ChatRole.ASSISTANT: 'assistant'>, content='Exactly — GEPA is basically genetic programming applied to LLM prompt optimization, but there are some subtleties when compared to traditional genetic algorithms (GAs) used for hyperparameter searches. Let’s break it down systematically.\\n\\n1. Core similarity: Evolutionary principle\\n\\nBoth traditional GAs and GEPA share the same genetic programming ideas:\\n\\nConcept\\tHyperparameter GA\\tGEPA\\nCandidate\\tA hyperparameter vector (learning rate, batch size, hidden units…)\\tA prompt or prompt template (often represented as text sequences or structured modules)\\nFitness evaluation\\tRun model training with the hyperparameters, measure validation accuracy/loss\\tRun the LLM with the candidate prompt, measure task performance (or sometimes multi-objective metrics)\\nSelection\\tTop-performing hyperparameter sets are selected\\tTop-performing prompts (Pareto front of multiple metrics or tasks) are selected\\nMutation & Crossover\\tPerturb hyperparameter values or recombine vectors\\tLLM-guided textual modifications (reflective edits) or recombination of prompt blocks\\nGeneration iteration\\tRepeat evaluation over generations\\tRepeat prompt evolution until performance converges\\n\\nSo the high-level mechanics—population, selection, mutation, crossover—are very similar.\\n\\n2. Key differences\\nAspect\\tHyperparameter GA\\tGEPA (LLM Optimization)\\nRepresentation of candidates\\tNumeric/continuous/discrete vectors\\tNatural language prompts or structured prompt modules\\nMutation/Crossover\\tSimple arithmetic or swap operations\\tSemantic, LLM-guided transformations based on reasoning or reflection (“why did it fail?”)\\nFitness evaluation\\tDeterministic scalar (accuracy, loss)\\tOften noisy, multi-dimensional; may involve multiple prompts/tasks or qualitative scoring\\nDiversity management\\tUsually rely on random mutation or tournament selection\\tUses Pareto front to maintain diverse high-performing prompts instead of one “best”\\nExploration vs Exploitation\\tControlled by mutation rate, crossover rate\\tControlled by reflection-guided modifications + Pareto sampling; can leverage reasoning from LLMs to direct exploration\\nFeedback\\tScalar metric only\\tRich text-based feedback (LLM reflections) → smarter candidate generation\\n3. Conceptual takeaway\\n\\nHyperparameter GAs: Numerical, often “blind” search guided purely by scalar fitness.\\n\\nGEPA: Language-aware GA. Candidates are semantic objects (prompts), and mutations are guided by the model itself, leveraging reflection to intelligently explore the space instead of random mutation.\\n\\nYou could say: GEPA is like a self-aware GA for language, where the model helps write its own “genes” based on observed failures, whereas traditional GAs just blindly mutate numbers.\\n\\nIf you want, I can draw a side-by-side diagram showing a GA for hyperparameters vs GEPA for prompts—it makes the difference very intuitive. Do you want me to do that?')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc0897c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"so the concepts of genetic programming applied to llm optimisation\n",
      "\n",
      "I know about genetic algorithms applied to hyperparameter searches \n",
      "\n",
      "what are the similiraties and differences?\"\n",
      "Main concept: GEPA (Genetic-Pareto Algorithm)\n",
      "    - Subconcept: Genetic programming applied to LLM optimization\n",
      "    - Subconcept: Similarities to traditional Genetic Algorithms (GAs)\n",
      "        - Supporting detail: Evolutionary principle\n",
      "            - Candidate representation\n",
      "            - Fitness evaluation\n",
      "            - Selection\n",
      "            - Mutation & Crossover\n",
      "            - Generation iteration\n",
      "    - Subconcept: Differences from traditional GAs\n",
      "        - Supporting detail: Representation of candidates\n",
      "        - Supporting detail: Mutation/Crossover\n",
      "        - Supporting detail: Fitness evaluation\n",
      "        - Supporting detail: Diversity management\n",
      "        - Supporting detail: Exploration vs Exploitation\n",
      "        - Supporting detail: Feedback\n"
     ]
    }
   ],
   "source": [
    "# Print the results of intermediate signals\n",
    "print(result.user_confusion_signals)\n",
    "print(result.concept_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "843fd065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Flashcard(front='What is the core similarity between traditional GAs and GEPA?', back='Both share the same evolutionary principle.', explanation='Both traditional GAs and GEPA are based on the same core principles of evolutionary algorithms: candidate representation, fitness evaluation, selection, mutation & crossover, and generation iteration. They both iteratively improve a population of candidates based on their performance.'), Flashcard(front='How are candidates represented in traditional GAs vs. GEPA?', back='Traditional GAs use numeric vectors; GEPA uses natural language prompts or structured prompt modules.', explanation='In traditional GAs for hyperparameter optimization, candidates are represented as numeric vectors of hyperparameters. In GEPA, candidates are natural language prompts or structured prompt modules that are designed to elicit desired behavior from an LLM. This difference in representation leads to different mutation and crossover strategies.'), Flashcard(front='How do mutation/crossover differ between traditional GAs and GEPA?', back='Traditional GAs use simple arithmetic or swap operations; GEPA uses semantic, LLM-guided transformations.', explanation='Traditional GAs use simple arithmetic operations or swap operations to mutate and crossover candidates. GEPA, on the other hand, uses LLM-guided transformations based on reasoning or reflection to modify prompts. This allows for more intelligent and context-aware exploration of the search space.'), Flashcard(front='How is fitness evaluation different in traditional GAs vs. GEPA?', back='Traditional GAs use deterministic scalar metrics; GEPA often uses noisy, multi-dimensional metrics.', explanation='In traditional GAs, fitness evaluation is typically based on a deterministic scalar metric like accuracy or loss. In GEPA, fitness evaluation can be more complex, involving multiple prompts/tasks or qualitative scoring, and is often noisy due to the inherent variability in LLM outputs.'), Flashcard(front='How is diversity managed in traditional GAs vs. GEPA?', back='Traditional GAs rely on random mutation or tournament selection; GEPA uses a Pareto front.', explanation='Traditional GAs typically rely on random mutation or tournament selection to maintain diversity in the population. GEPA uses a Pareto front to maintain a diverse set of high-performing prompts, ensuring that the algorithm explores a wider range of potential solutions.'), Flashcard(front='How is exploration vs. exploitation controlled in traditional GAs vs. GEPA?', back='Traditional GAs use mutation and crossover rates; GEPA uses reflection-guided modifications and Pareto sampling.', explanation='In traditional GAs, exploration vs. exploitation is controlled by adjusting the mutation and crossover rates. In GEPA, exploration is guided by reflection-guided modifications and Pareto sampling, which allows the algorithm to leverage reasoning from LLMs to direct exploration more effectively.'), Flashcard(front='What type of feedback is used in traditional GAs vs. GEPA?', back='Traditional GAs use scalar metrics; GEPA uses rich text-based feedback.', explanation='Traditional GAs rely on scalar metrics as feedback, such as accuracy or loss. GEPA leverages rich text-based feedback in the form of LLM reflections, which provides more detailed information about why a particular prompt failed or succeeded. This allows for smarter candidate generation.'), Flashcard(front='What is the conceptual takeaway comparing hyperparameter GAs and GEPA?', back=\"Hyperparameter GAs are numerical and 'blind'; GEPA is language-aware and uses model reflection.\", explanation=\"Hyperparameter GAs perform a numerical, often 'blind' search guided purely by scalar fitness. GEPA, on the other hand, is a language-aware GA where candidates are semantic objects (prompts), and mutations are guided by the model itself, leveraging reflection to intelligently explore the space instead of random mutation. GEPA is like a self-aware GA for language.\")]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import dspy\n",
    "\n",
    "# Load the conversation object from the pickle file\n",
    "file_path = r'C:\\Users\\lokar\\flashcardgen\\fcg\\development\\conversation.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    conversation = pickle.load(f)\n",
    "\n",
    "# Test individual signatures\n",
    "ctf = ConversationToFlashcards()\n",
    "result = ctf(conversation=conversation)\n",
    "\n",
    "# Print the results\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a85914c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are candidates represented in traditional GAs vs. GEPA?'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1].front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "577cabb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traditional GAs use numeric vectors; GEPA uses natural language prompts or structured prompt modules.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1].back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0d3a8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In traditional GAs for hyperparameter optimization, candidates are represented as numeric vectors of hyperparameters. In GEPA, candidates are natural language prompts or structured prompt modules that are designed to elicit desired behavior from an LLM. This difference in representation leads to different mutation and crossover strategies.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1].explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984c515",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- if each card is verified, too many llm calls add to card delay generation\n",
    "- received junk output when I let all the verification calls run for some reason (talked about mandelbrots?? instead of gepa)\n",
    "- Both the class docstring and the desc parameter in output fields are incorporated into the prompt that guides the LLM's generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf186bb5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
